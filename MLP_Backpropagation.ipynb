{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de MLP_Backpropagation_mnist_fasshion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhHATMnuACOsEq+R7giPt1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Clasifying-sentiment-of-reviews/blob/master/MLP_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rgAk6iudSUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cctRvJDdbt2",
        "colab_type": "text"
      },
      "source": [
        "# Multi-layer Perceptron and Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikTCJYrHs5B_",
        "colab_type": "text"
      },
      "source": [
        "## Multi-layer Perceptron "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFFMDpggdmmL",
        "colab_type": "text"
      },
      "source": [
        "An MLP is composed of one input layer, one or more hidden layers of [artificial neurons](https://en.wikipedia.org/wiki/Artificial_neuron)(particularly TLUs or threshold logic units) and one final layer of TLUs called the output layer. Every layer except the output layer includes a bias neuron and is fully connected to the next layer.\n",
        "\n",
        "\n",
        "\n",
        "![texto alternativo](https://www.oreilly.com/library/view/getting-started-with/9781786468574/graphics/B05474_04_05.jpg)\n",
        "\n",
        "The signal flows only in one direction(from the inputs to the outputs), so this architecture is an example of a feedforward neural network\n",
        "\n",
        "When an ANN contains a lot of hidden layers is known as a deep neural network and these networks are the base of the study of Deep learning, and more generally models containing deep stacks of computations.\n",
        "\n",
        "The interest for these networks begun in 1986 when the backpropagation training algorithm was introduced. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__NuADijs_wM",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "The backpropagation algorithm is based in two passes through the network(one forward, one backward). Let's see it in detail.\n",
        "\n",
        "\n",
        "- It handles one mini-batch at a time, this is, it goes through to the full training set multiple times. Each pass is an epoch.\n",
        "\n",
        "\n",
        "- Each mini-batch is passed to the input layer, which sends it to the first hidden layer. The computes the output of all the neurons in this layer and passes the result on to the next layer. This process is repeated until getting the output of the last layer. This is the forward pass. It is like making a prediction, except all intermediate results are preserved for the backward pass.\n",
        "\n",
        "\n",
        "- The algorithm measures the network's output error, it uses a loss function that compares the desired output and the actual output of the network and returns some measure of the error.\n",
        "\n",
        "- Then it goes through each layer in reverse(backward pass) computing how much each output connection contributed to the error(applying chain rule) and finally tweaks the weights of the connections to reduce the error(Gradient Descent step).\n",
        "\n",
        "![texto alternativo](https://i.stack.imgur.com/7Ui1C.png)\n",
        "\n",
        "Note: It is important to initialize all the hidden layer's connection weight randomly.\n",
        "\n",
        "The authors of this algorithm replaced the step function with the logistic function, which has a well-defined nonzero derivative, allowing Gradient Descent to make progress at every step. Backpropagation works well with other activation functions as well, like hyperbolic tangent function, Relu...\n",
        "\n",
        "But why do we need activation functions between layers?. If you apply chain rule on several linear transformations you will get a linear transformation. If you do not put these functions, a deep stack of layers is equivalent to a single layer and you cannot solve very complex problems with that. The purpose of the activation function is to introduce non-linearity into the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWxK0Mntdb9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZII9rUndcHc",
        "colab_type": "text"
      },
      "source": [
        "# Regression MLPs\n",
        "\n",
        "MLPs can be used for regression tasks. For univariate regression(predict a single value) you need a single output neuron(the predicted value). For multivariate regression(predict multiple values at once), you need one output neuron per output dimension.\n",
        "\n",
        "In general, when building an MLP for regression it is not usually used activation function for the output neurons(they are free to output any range of values). However, if you want that outputs will always be positive, you can use the ReLU activation function, or softplus in the output layer. Finally, if you want that the predictions will fall within a given range of values you can use the logistic function or hyperbolic tangent, and scale the labels to the adequate range.\n",
        "\n",
        "The loss function typically is the mean squared error, but there are others like mean absolute error or Huber loss(a good choice if the training set has a lot of outliers).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jga8m59p9PjJ",
        "colab_type": "text"
      },
      "source": [
        "# Clasification MLPs\n",
        "\n",
        "MLPs can be used for classification tasks. For binary classification, you need a single output neuron using the logistic activation function(the predicted value will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class).\n",
        "\n",
        "For multi-label binary classification, for example, predicts whether the incoming email is spam or no-spam, and simultaneously predicts whether it is an urgent or non-urgent email. In this case, you would need two output neurons, both using the logistic function: the first output will be the probability estimated that the email is spam and the second the probability estimated that the email is urgent. Generally, you can use one output neuron for each positive class.\n",
        "\n",
        "For multiclass classification, this is when target variable can belong to more than two labels(for example classes 0 through 9 for digit image classification), you need to have one output neuron per class and you can use the softmax activation function for the whole output layer(softmax activation function ensure that all estimated probabilities are between 0 and 1).\n",
        "\n",
        "Regarding the loss function, we are predicting probability distributions, the cross-entropy is generally a good option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNfFWL8sdcU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahxS3gRPAz0J",
        "colab_type": "text"
      },
      "source": [
        "# MNIST fashion dataset\n",
        "\n",
        "![texto alternativo](https://camo.githubusercontent.com/a2e4e2eb7beebba0496cdf0bd08761636b6ecddd/68747470733a2f2f692e6779617a6f2e636f6d2f30666237313934393761386236366164393938643132333561663237643930312e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le3Lrps5dchz",
        "colab_type": "code",
        "outputId": "837d7ad0-33f3-4678-e147-7f60a5c31264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "import keras\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 4us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcQAyTkmE-_u",
        "colab_type": "code",
        "outputId": "93495bac-0b86-4b1e-a8d8-544d68673824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(X_train_full.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvkeIOXNEvFp",
        "colab_type": "text"
      },
      "source": [
        "The dataset is already split into a training set and a test set.\n",
        "\n",
        "Since we are going to train the neural network using Gradient Descent, we must scale the input feature. We just scale the pixel intensity down to the 0-1 range by dividing them by 255.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKR836EmdckV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_full = X_train_full / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFiALH-nFqj9",
        "colab_type": "text"
      },
      "source": [
        "We need the list of classes names "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBTAMAuYdcm0",
        "colab_type": "code",
        "outputId": "e3944867-16ee-4bd9-8cc4-a094a9c75821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "# let's see what class represent the first image\n",
        "class_names[y_train_full[0]]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ankle boot'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gp3yyYbGWdq",
        "colab_type": "text"
      },
      "source": [
        "### Creating a model using Sequential API\n",
        "Now let’s build the neural network. It is a classification MLP with two hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haSCK_lFdcpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28,28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\"),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmMRbXJQUo5M",
        "colab_type": "text"
      },
      "source": [
        "The first line creates a  Sequential model. This is the simplest kind of Keras model, for neural networks that are just composed of a single stack of layers, connected sequentially. This is called the sequential API.\n",
        "\n",
        "Next, pass a list of layers to the model. The first layer is a  Flatten layer used to convert each input image into a 1D array: if it receives input data X, it computes  X.reshape(-1, 1). This layer does not have any parameters, it is just to do some simple preprocessing. Since it is the first layer, you should specify the input_shape: this does not include the batch size, only the shape of the instances. Alternatively, you could add a  keras.layers.InputLayer as the first layer, setting  shape=[28,28].\n",
        "\n",
        "\n",
        "Next, we add a  Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each  Dense layer manages its weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron).\n",
        "\n",
        "Next a second  Dense hidden layer with 100 neurons, also using the ReLU activation function. \n",
        "\n",
        "Finally, a  Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSP-NGA7dcsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D76fmQPCYx9P",
        "colab_type": "text"
      },
      "source": [
        "The model’s  summary()  method displays all the model’s layers, including each layer’s name, its output shape, and its number of parameters. The summary ends with the total number of parameters, trainable and non-trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtK_xPzbdcxM",
        "colab_type": "code",
        "outputId": "76642db4-a27f-43ad-8114-65ee0e9578bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFGTSdzlZ0ea",
        "colab_type": "text"
      },
      "source": [
        "Note that  Dense layers often have a  lot of parameters. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which reaches to 235,500 parameters! This gives the model flexibility to fit the training data, but also the model runs the risk of overfitting, especially when you do not have a lot of training data.\n",
        "\n",
        "You can easily get a model's list of layers or all the parameters of a specific layer(for a de4nse layer includes both the connection weight and the bias)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJFdmQhNdczz",
        "colab_type": "code",
        "outputId": "4927019b-712c-4394-9b82-975c75bbe515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(model.layers)\n",
        "dense_3 = model.layers[3]\n",
        "print(dense_3)\n",
        "weigth, biases = dense_3.get_weights()\n",
        "print(weigth[0:5], biases[0:5])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<keras.layers.core.Flatten object at 0x7f66e1232908>, <keras.layers.core.Dense object at 0x7f66e12322b0>, <keras.layers.core.Dense object at 0x7f66e1232eb8>, <keras.layers.core.Dense object at 0x7f66e1239080>]\n",
            "<keras.layers.core.Dense object at 0x7f66e1239080>\n",
            "[[-0.08344731  0.05062839 -0.1473903  -0.10197224  0.09504732 -0.177553\n",
            "   0.09958991  0.0007491  -0.0371723   0.13009366]\n",
            " [-0.1858958  -0.19105196 -0.15192553 -0.10910078  0.18888274  0.04740137\n",
            "  -0.08409178 -0.13097033 -0.20229188 -0.12336727]\n",
            " [-0.13278203  0.06738234  0.03454864  0.07392576 -0.21242766  0.07000265\n",
            "   0.18933678  0.12681904  0.20364097 -0.18167685]\n",
            " [-0.08463646  0.1950235   0.1426683   0.08156613  0.06967828  0.04318601\n",
            "   0.17055169 -0.15584442  0.05400521 -0.05336803]\n",
            " [-0.00631903  0.01425207 -0.13982254 -0.13407125  0.12167487  0.07378361\n",
            "   0.1043663   0.01090299  0.18947926 -0.22044963]] [0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2OaMkEwv3ig",
        "colab_type": "text"
      },
      "source": [
        "Notice that the Dense layer initialized the connection weights randomly, and biases were initialized to zeros. You can use another initialization method.\n",
        "\n",
        "The shape of the weight matrix depends on the number of inputs. It is recommended to specify the input_saphe when creating the first layer in a Sequential model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhFdygkGdcwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrGQrHWPG_Ci",
        "colab_type": "text"
      },
      "source": [
        "### Compiling the model\n",
        "\n",
        "We call compile method to specify the loss function and the optimizer to use. You can specify extra metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YawwgW4jG6g8",
        "colab_type": "code",
        "outputId": "8fe26812-0fc2-4385-b228-87f799a07e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dumP9hJkxZCl",
        "colab_type": "text"
      },
      "source": [
        "The optimizer \"sgd\" will train the model using simple Stochastic Gradient Descent. Specifically, it will perform the backpropagation algorithm described earlier. There are other efficient optimizers(to improve gradient descent part).\n",
        "We use measure \"accurary\" during training and evaluation for our clasification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoIZg-WqG67h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FmBcI0pHSa6",
        "colab_type": "text"
      },
      "source": [
        "### Training and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLwo-Z5sG6-p",
        "colab_type": "code",
        "outputId": "630e4a3d-3cfc-47fa-f89e-67e5a3b395c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train_full, y_train_full, epochs=30, validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/30\n",
            "48000/48000 [==============================] - 7s 144us/step - loss: 0.7310 - acc: 0.7572 - val_loss: 0.5187 - val_acc: 0.8193\n",
            "Epoch 2/30\n",
            "48000/48000 [==============================] - 8s 157us/step - loss: 0.5063 - acc: 0.8229 - val_loss: 0.4744 - val_acc: 0.8340\n",
            "Epoch 3/30\n",
            "48000/48000 [==============================] - 8s 162us/step - loss: 0.4669 - acc: 0.8356 - val_loss: 0.4619 - val_acc: 0.8341\n",
            "Epoch 4/30\n",
            "48000/48000 [==============================] - 7s 156us/step - loss: 0.4392 - acc: 0.8457 - val_loss: 0.4334 - val_acc: 0.8495\n",
            "Epoch 5/30\n",
            "48000/48000 [==============================] - 7s 155us/step - loss: 0.4207 - acc: 0.8522 - val_loss: 0.4350 - val_acc: 0.8438\n",
            "Epoch 6/30\n",
            "48000/48000 [==============================] - 7s 151us/step - loss: 0.4078 - acc: 0.8556 - val_loss: 0.4388 - val_acc: 0.8386\n",
            "Epoch 7/30\n",
            "48000/48000 [==============================] - 7s 156us/step - loss: 0.3950 - acc: 0.8614 - val_loss: 0.3986 - val_acc: 0.8578\n",
            "Epoch 8/30\n",
            "48000/48000 [==============================] - 7s 156us/step - loss: 0.3850 - acc: 0.8633 - val_loss: 0.4168 - val_acc: 0.8515\n",
            "Epoch 9/30\n",
            "48000/48000 [==============================] - 7s 149us/step - loss: 0.3753 - acc: 0.8664 - val_loss: 0.4205 - val_acc: 0.8435\n",
            "Epoch 10/30\n",
            "48000/48000 [==============================] - 8s 156us/step - loss: 0.3665 - acc: 0.8688 - val_loss: 0.3753 - val_acc: 0.8666\n",
            "Epoch 11/30\n",
            "48000/48000 [==============================] - 8s 157us/step - loss: 0.3601 - acc: 0.8729 - val_loss: 0.4018 - val_acc: 0.8560\n",
            "Epoch 12/30\n",
            "48000/48000 [==============================] - 7s 156us/step - loss: 0.3505 - acc: 0.8758 - val_loss: 0.3819 - val_acc: 0.8647\n",
            "Epoch 13/30\n",
            "48000/48000 [==============================] - 7s 155us/step - loss: 0.3465 - acc: 0.8760 - val_loss: 0.3532 - val_acc: 0.8726\n",
            "Epoch 14/30\n",
            "48000/48000 [==============================] - 7s 149us/step - loss: 0.3389 - acc: 0.8782 - val_loss: 0.3522 - val_acc: 0.8738\n",
            "Epoch 15/30\n",
            "48000/48000 [==============================] - 7s 154us/step - loss: 0.3324 - acc: 0.8797 - val_loss: 0.3489 - val_acc: 0.8755\n",
            "Epoch 16/30\n",
            "48000/48000 [==============================] - 8s 173us/step - loss: 0.3287 - acc: 0.8808 - val_loss: 0.3480 - val_acc: 0.8742\n",
            "Epoch 17/30\n",
            "48000/48000 [==============================] - 7s 149us/step - loss: 0.3216 - acc: 0.8840 - val_loss: 0.3722 - val_acc: 0.8716\n",
            "Epoch 18/30\n",
            "48000/48000 [==============================] - 7s 156us/step - loss: 0.3165 - acc: 0.8851 - val_loss: 0.3632 - val_acc: 0.8683\n",
            "Epoch 19/30\n",
            "48000/48000 [==============================] - 7s 154us/step - loss: 0.3107 - acc: 0.8875 - val_loss: 0.3490 - val_acc: 0.8748\n",
            "Epoch 20/30\n",
            "48000/48000 [==============================] - 7s 155us/step - loss: 0.3067 - acc: 0.8897 - val_loss: 0.3376 - val_acc: 0.8793\n",
            "Epoch 21/30\n",
            "48000/48000 [==============================] - 7s 154us/step - loss: 0.3004 - acc: 0.8893 - val_loss: 0.3637 - val_acc: 0.8728\n",
            "Epoch 22/30\n",
            "48000/48000 [==============================] - 7s 152us/step - loss: 0.2960 - acc: 0.8923 - val_loss: 0.3399 - val_acc: 0.8782\n",
            "Epoch 23/30\n",
            "48000/48000 [==============================] - 7s 154us/step - loss: 0.2943 - acc: 0.8930 - val_loss: 0.3580 - val_acc: 0.8710\n",
            "Epoch 24/30\n",
            "48000/48000 [==============================] - 8s 157us/step - loss: 0.2882 - acc: 0.8951 - val_loss: 0.3445 - val_acc: 0.8767\n",
            "Epoch 25/30\n",
            "48000/48000 [==============================] - 7s 149us/step - loss: 0.2836 - acc: 0.8971 - val_loss: 0.3515 - val_acc: 0.8766\n",
            "Epoch 26/30\n",
            "48000/48000 [==============================] - 8s 157us/step - loss: 0.2801 - acc: 0.8981 - val_loss: 0.3371 - val_acc: 0.8769\n",
            "Epoch 27/30\n",
            "48000/48000 [==============================] - 8s 156us/step - loss: 0.2764 - acc: 0.8990 - val_loss: 0.3542 - val_acc: 0.8747\n",
            "Epoch 28/30\n",
            "48000/48000 [==============================] - 7s 153us/step - loss: 0.2734 - acc: 0.8989 - val_loss: 0.3284 - val_acc: 0.8808\n",
            "Epoch 29/30\n",
            "48000/48000 [==============================] - 7s 154us/step - loss: 0.2711 - acc: 0.9002 - val_loss: 0.3442 - val_acc: 0.8772\n",
            "Epoch 30/30\n",
            "48000/48000 [==============================] - 7s 152us/step - loss: 0.2660 - acc: 0.9034 - val_loss: 0.3278 - val_acc: 0.8802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeVj0g-czJZB",
        "colab_type": "text"
      },
      "source": [
        "At each epoch during training, Keras displays the number of instances processes, the mean training time per sample, the loss and accuracy both on the training and validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R953dkr73_R4",
        "colab_type": "text"
      },
      "source": [
        "If training set was unbulanced, with some classes being overrepresented and others underrepresented can be useful to set the class_weight argument when calling the fit() method(giving larger weight to underrepresented classes and a lower weight to overrepresented classes)\n",
        "\n",
        "The fit() method returns a History object which contains the training parameters, list of epochs and a dictionary which contains the loss and extra metrics it measured at the end of each epoch on both training and validation set. We can use this measures to plot the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuy-FLaMG6eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMrCRHI2G6cP",
        "colab_type": "code",
        "outputId": "1bb43394-6249-4140-a36a-75cd3bf6be56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1)\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2o1CXsp2LQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT5kn2P92Lfv",
        "colab_type": "text"
      },
      "source": [
        "We see that both the training and validation accuracy increase during training, while loss decrease. The validation curves are quite close to the training curves, which means that there is not too much overfitting.\n",
        "\n",
        "If you are not satisfied with the performance of the model, you can tune the model's hyperparameter and train the model again. You can modify the number of neurons per layer, the number of layers, the types of activation functiuons for each hidden layer, the number of epochs, the batch size(by default it is 32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpaCTu7GG6aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKe2a-DYG6T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}